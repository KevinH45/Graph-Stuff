{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from graph_clustering import A_binarize, creating_label\n",
    "from graph_features import graph_norm\n",
    "from graph_layers import GraphConvolution, GraphLinear, InnerProductDecoder\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "import time\n",
    "from preprocessing_functions import preprocess_data\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that the 3 loss function is defined and we include the new proposed decoder model\n",
    "# Experiment stuff\n",
    "subject_num = 149  # Size of dataset\n",
    "run_num = 1  # Number of \"experiences\" of each task (this should be one)\n",
    "num_electrodes = 60  # Number of EEG electrodes (this is excluding [\"Iz\", \"I1\", \"I2\", \"Resp\", \"PO4\", \"PO3\", \"FT9\", \"Status\"])\n",
    "data_length = 60500  # Minimum data length, cut off anything outside this data length\n",
    "BASE_PATH = r\"C:\\Users\\timmy\\Downloads\\park-eeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the dataset and labels\n",
    "def load_dataset():\n",
    "    # Format is sub-NUM_task-Rest_eeg.edf\n",
    "    data = []\n",
    "    for subject in range(subject_num):\n",
    "        subject_list = []\n",
    "        for run in range(run_num):\n",
    "            file_name = r\"\\sub-{}_task-Rest_eeg.edf\".format(str(subject + 1).zfill(3))\n",
    "            try:\n",
    "                f = pyedflib.EdfReader(BASE_PATH + file_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            electrode_list = []\n",
    "            for electrode in range(num_electrodes):  # Electrodes are zero-indexed\n",
    "                # Iz, I1, I2, PO4, PO3, FT9 is not present in all subjects... exclude\n",
    "                # Status/Resp... I'm assuming is ground/reference electrode\n",
    "                # Regardless, both status/resp are not present in all subjects\n",
    "                if f.getLabel(electrode) in (\"Iz\", \"I1\", \"I2\", \"Resp\", \"PO4\", \"PO3\", \"FT9\", \"Status\"):\n",
    "                    continue\n",
    "                electrode_list.append(f.readSignal(electrode)[:data_length])\n",
    "            subject_list.append(electrode_list)\n",
    "            f._close()\n",
    "            del f  # Don't read all files into memory\n",
    "\n",
    "        data.append(subject_list)\n",
    "        print(\"Subject\", subject, \"done!\")\n",
    "\n",
    "    raw_labels = pd.read_csv(\"participants.tsv\", sep=\"\\t\")\n",
    "    # Get rid of #68 and add labels\n",
    "    new_data = []\n",
    "    labels = []\n",
    "    for index, subject in enumerate(data):\n",
    "\n",
    "        if not subject:\n",
    "            continue\n",
    "\n",
    "        new_data.append(subject)\n",
    "\n",
    "        query = \"sub-\" + str(index + 1).zfill(3)\n",
    "        results = raw_labels[(raw_labels[\"participant_id\"] == query)]\n",
    "        labels.append(results.iloc[0][\"GROUP\"])\n",
    "\n",
    "        print(\"Label\", index, \"done!\")\n",
    "\n",
    "    return np.array(new_data), np.array([i == \"PD\" for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0 done!\n",
      "Subject 1 done!\n",
      "Subject 2 done!\n",
      "Subject 3 done!\n",
      "Subject 4 done!\n",
      "Subject 5 done!\n",
      "Subject 6 done!\n",
      "Subject 7 done!\n",
      "Subject 8 done!\n",
      "Subject 9 done!\n",
      "Subject 10 done!\n",
      "Subject 11 done!\n",
      "Subject 12 done!\n",
      "Subject 13 done!\n",
      "Subject 14 done!\n",
      "Subject 15 done!\n",
      "Subject 16 done!\n",
      "Subject 17 done!\n",
      "Subject 18 done!\n",
      "Subject 19 done!\n",
      "Subject 20 done!\n",
      "Subject 21 done!\n",
      "Subject 22 done!\n",
      "Subject 23 done!\n",
      "Subject 24 done!\n",
      "Subject 25 done!\n",
      "Subject 26 done!\n",
      "Subject 27 done!\n",
      "Subject 28 done!\n",
      "Subject 29 done!\n",
      "Subject 30 done!\n",
      "Subject 31 done!\n",
      "Subject 32 done!\n",
      "Subject 33 done!\n",
      "Subject 34 done!\n",
      "Subject 35 done!\n",
      "Subject 36 done!\n",
      "Subject 37 done!\n",
      "Subject 38 done!\n",
      "Subject 39 done!\n",
      "Subject 40 done!\n",
      "Subject 41 done!\n",
      "Subject 42 done!\n",
      "Subject 43 done!\n",
      "Subject 44 done!\n",
      "Subject 45 done!\n",
      "Subject 46 done!\n",
      "Subject 47 done!\n",
      "Subject 48 done!\n",
      "Subject 49 done!\n",
      "Subject 50 done!\n",
      "Subject 51 done!\n",
      "Subject 52 done!\n",
      "Subject 53 done!\n",
      "Subject 54 done!\n",
      "Subject 55 done!\n",
      "Subject 56 done!\n",
      "Subject 57 done!\n",
      "Subject 58 done!\n",
      "Subject 59 done!\n",
      "Subject 60 done!\n",
      "Subject 61 done!\n",
      "Subject 62 done!\n",
      "Subject 63 done!\n",
      "Subject 64 done!\n",
      "Subject 65 done!\n",
      "Subject 66 done!\n",
      "C:\\Users\\timmy\\Downloads\\park-eeg\\sub-068_task-Rest_eeg.edf: the file is not EDF(+) or BDF(+) compliant (Physical Minimum)\n",
      "Subject 67 done!\n",
      "Subject 68 done!\n",
      "Subject 69 done!\n",
      "Subject 70 done!\n",
      "Subject 71 done!\n",
      "Subject 72 done!\n",
      "Subject 73 done!\n",
      "Subject 74 done!\n",
      "Subject 75 done!\n",
      "Subject 76 done!\n",
      "Subject 77 done!\n",
      "Subject 78 done!\n",
      "Subject 79 done!\n",
      "Subject 80 done!\n",
      "Subject 81 done!\n",
      "Subject 82 done!\n",
      "Subject 83 done!\n",
      "Subject 84 done!\n",
      "Subject 85 done!\n",
      "Subject 86 done!\n",
      "Subject 87 done!\n",
      "Subject 88 done!\n",
      "Subject 89 done!\n",
      "Subject 90 done!\n",
      "Subject 91 done!\n",
      "Subject 92 done!\n",
      "Subject 93 done!\n",
      "Subject 94 done!\n",
      "Subject 95 done!\n",
      "Subject 96 done!\n",
      "Subject 97 done!\n",
      "Subject 98 done!\n",
      "Subject 99 done!\n",
      "Subject 100 done!\n",
      "Subject 101 done!\n",
      "Subject 102 done!\n",
      "Subject 103 done!\n",
      "Subject 104 done!\n",
      "Subject 105 done!\n",
      "Subject 106 done!\n",
      "Subject 107 done!\n",
      "Subject 108 done!\n",
      "Subject 109 done!\n",
      "Subject 110 done!\n",
      "Subject 111 done!\n",
      "Subject 112 done!\n",
      "Subject 113 done!\n",
      "Subject 114 done!\n",
      "Subject 115 done!\n",
      "Subject 116 done!\n",
      "Subject 117 done!\n",
      "Subject 118 done!\n",
      "Subject 119 done!\n",
      "Subject 120 done!\n",
      "Subject 121 done!\n",
      "Subject 122 done!\n",
      "Subject 123 done!\n",
      "Subject 124 done!\n",
      "Subject 125 done!\n",
      "Subject 126 done!\n",
      "Subject 127 done!\n",
      "Subject 128 done!\n",
      "Subject 129 done!\n",
      "Subject 130 done!\n",
      "Subject 131 done!\n",
      "Subject 132 done!\n",
      "Subject 133 done!\n",
      "Subject 134 done!\n",
      "Subject 135 done!\n",
      "Subject 136 done!\n",
      "Subject 137 done!\n",
      "Subject 138 done!\n",
      "Subject 139 done!\n",
      "Subject 140 done!\n",
      "Subject 141 done!\n",
      "Subject 142 done!\n",
      "Subject 143 done!\n",
      "Subject 144 done!\n",
      "Subject 145 done!\n",
      "Subject 146 done!\n",
      "Subject 147 done!\n",
      "Subject 148 done!\n",
      "Label 0 done!\n",
      "Label 1 done!\n",
      "Label 2 done!\n",
      "Label 3 done!\n",
      "Label 4 done!\n",
      "Label 5 done!\n",
      "Label 6 done!\n",
      "Label 7 done!\n",
      "Label 8 done!\n",
      "Label 9 done!\n",
      "Label 10 done!\n",
      "Label 11 done!\n",
      "Label 12 done!\n",
      "Label 13 done!\n",
      "Label 14 done!\n",
      "Label 15 done!\n",
      "Label 16 done!\n",
      "Label 17 done!\n",
      "Label 18 done!\n",
      "Label 19 done!\n",
      "Label 20 done!\n",
      "Label 21 done!\n",
      "Label 22 done!\n",
      "Label 23 done!\n",
      "Label 24 done!\n",
      "Label 25 done!\n",
      "Label 26 done!\n",
      "Label 27 done!\n",
      "Label 28 done!\n",
      "Label 29 done!\n",
      "Label 30 done!\n",
      "Label 31 done!\n",
      "Label 32 done!\n",
      "Label 33 done!\n",
      "Label 34 done!\n",
      "Label 35 done!\n",
      "Label 36 done!\n",
      "Label 37 done!\n",
      "Label 38 done!\n",
      "Label 39 done!\n",
      "Label 40 done!\n",
      "Label 41 done!\n",
      "Label 42 done!\n",
      "Label 43 done!\n",
      "Label 44 done!\n",
      "Label 45 done!\n",
      "Label 46 done!\n",
      "Label 47 done!\n",
      "Label 48 done!\n",
      "Label 49 done!\n",
      "Label 50 done!\n",
      "Label 51 done!\n",
      "Label 52 done!\n",
      "Label 53 done!\n",
      "Label 54 done!\n",
      "Label 55 done!\n",
      "Label 56 done!\n",
      "Label 57 done!\n",
      "Label 58 done!\n",
      "Label 59 done!\n",
      "Label 60 done!\n",
      "Label 61 done!\n",
      "Label 62 done!\n",
      "Label 63 done!\n",
      "Label 64 done!\n",
      "Label 65 done!\n",
      "Label 66 done!\n",
      "Label 68 done!\n",
      "Label 69 done!\n",
      "Label 70 done!\n",
      "Label 71 done!\n",
      "Label 72 done!\n",
      "Label 73 done!\n",
      "Label 74 done!\n",
      "Label 75 done!\n",
      "Label 76 done!\n",
      "Label 77 done!\n",
      "Label 78 done!\n",
      "Label 79 done!\n",
      "Label 80 done!\n",
      "Label 81 done!\n",
      "Label 82 done!\n",
      "Label 83 done!\n",
      "Label 84 done!\n",
      "Label 85 done!\n",
      "Label 86 done!\n",
      "Label 87 done!\n",
      "Label 88 done!\n",
      "Label 89 done!\n",
      "Label 90 done!\n",
      "Label 91 done!\n",
      "Label 92 done!\n",
      "Label 93 done!\n",
      "Label 94 done!\n",
      "Label 95 done!\n",
      "Label 96 done!\n",
      "Label 97 done!\n",
      "Label 98 done!\n",
      "Label 99 done!\n",
      "Label 100 done!\n",
      "Label 101 done!\n",
      "Label 102 done!\n",
      "Label 103 done!\n",
      "Label 104 done!\n",
      "Label 105 done!\n",
      "Label 106 done!\n",
      "Label 107 done!\n",
      "Label 108 done!\n",
      "Label 109 done!\n",
      "Label 110 done!\n",
      "Label 111 done!\n",
      "Label 112 done!\n",
      "Label 113 done!\n",
      "Label 114 done!\n",
      "Label 115 done!\n",
      "Label 116 done!\n",
      "Label 117 done!\n",
      "Label 118 done!\n",
      "Label 119 done!\n",
      "Label 120 done!\n",
      "Label 121 done!\n",
      "Label 122 done!\n",
      "Label 123 done!\n",
      "Label 124 done!\n",
      "Label 125 done!\n",
      "Label 126 done!\n",
      "Label 127 done!\n",
      "Label 128 done!\n",
      "Label 129 done!\n",
      "Label 130 done!\n",
      "Label 131 done!\n",
      "Label 132 done!\n",
      "Label 133 done!\n",
      "Label 134 done!\n",
      "Label 135 done!\n",
      "Label 136 done!\n",
      "Label 137 done!\n",
      "Label 138 done!\n",
      "Label 139 done!\n",
      "Label 140 done!\n",
      "Label 141 done!\n",
      "Label 142 done!\n",
      "Label 143 done!\n",
      "Label 144 done!\n",
      "Label 145 done!\n",
      "Label 146 done!\n",
      "Label 147 done!\n",
      "Label 148 done!\n"
     ]
    }
   ],
   "source": [
    "Binary = True\n",
    "partial_subject = False\n",
    "part_channel = False\n",
    "verbose = True\n",
    "nb_run = 5  # 5-fold cross validation\n",
    "step = 160 * 0 + 80  # 1-window*alpha%\n",
    "Fs = 500  # Sampling frequency (hz)\n",
    "Ts = 1 / Fs  # Time in seconds ??\n",
    "\n",
    "# Initialized numpy arrays for in-train data\n",
    "accuracy = np.zeros((nb_run, 1))\n",
    "accuracy2 = np.zeros((nb_run, 1))\n",
    "Computational_time = np.zeros((nb_run, 1))\n",
    "num_epoch = np.zeros((nb_run, 1))\n",
    "full_time = np.zeros((nb_run, 1))\n",
    "roc_auc = np.zeros((nb_run, 1))\n",
    "EER = np.zeros((nb_run, 1))\n",
    "\n",
    "x_raw_all, labels = load_dataset()  # import data for all subject \n",
    "\n",
    "loss_function = 3  # 3 loss function is defined\n",
    "decoder_adj = True  # include new decoder model\n",
    "\n",
    "FLAGS_features = False  # Whether or not to include pre-defined features\n",
    "features_init_train = None\n",
    "features_init_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invlogit(z):  # convert decoded adjancy matrix to original space\n",
    "    return 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "def Adj_matrix(train_x, test_x):\n",
    "    if (Binary):\n",
    "        # Change weighted matrix to binary matrix with threshold\n",
    "        percentile = 0.75\n",
    "        adj_train = A_binarize(A_matrix=train_x, percent=percentile, sparse=False)\n",
    "        adj_test = A_binarize(A_matrix=test_x, percent=percentile, sparse=False)\n",
    "        # sparse matrix\n",
    "    else:\n",
    "        adj_train = deepcopy(train_x)\n",
    "        adj_test = deepcopy(test_x)\n",
    "    # consider part of the graph\n",
    "    print(\"sparsity: \", scipy.sparse.issparse(adj_train[9]))  # check sparsity\n",
    "    print(\"rank: \", np.linalg.matrix_rank(adj_train[9]))  # check matrix rank\n",
    "    return adj_train, adj_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA...\n",
      "Start ICA\n",
      "END ICA\n",
      "ADF Statistic: -54.916290\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.431\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n",
      "Creating brain graph....\n",
      "sparsity:  False\n",
      "rank:  57\n",
      "Preprocessing...\n"
     ]
    }
   ],
   "source": [
    "run = 0\n",
    "dropout_rate = 0.5\n",
    "t_start = time.time()\n",
    "\n",
    "print(\"Applying ICA...\")\n",
    "sec = 12  # Number of seconds in window?\n",
    "\n",
    "train_x, test_x, y_train, y_test = preprocess_data(x_raw_all[:, 0], labels, run, Fs,\n",
    "                                                    filt=False, ICA=True, A_Matrix='cov', sec=sec)\n",
    "\n",
    "print(\"Creating brain graph....\")\n",
    "adj_train, adj_test = Adj_matrix(train_x, test_x)  # Creating functional connectivity graph\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "# Compute number of nodes\n",
    "num_nodes = adj_train.shape[1]\n",
    "\n",
    "# If features are not used, replace feature matrix by identity matrix\n",
    "I_train = np.tile(np.eye(adj_train.shape[1]), adj_train.shape[0]).T.reshape(-1, adj_train.shape[1],\n",
    "                                                                            adj_train.shape[1])\n",
    "I_test = np.tile(np.eye(adj_test.shape[1]), adj_test.shape[0]).T.reshape(-1, adj_test.shape[1], adj_test.shape[1])\n",
    "\n",
    "features = np.ones((adj_train.shape[0], adj_train.shape[1], 1))\n",
    "\n",
    "# Preprocessing on node features\n",
    "num_features = features.shape[2]\n",
    "features_nonzero = np.count_nonzero(features) // features.shape[0]\n",
    "\n",
    "# Normalization and preprocessing on adjacency matrix\n",
    "adj_norm_train = graph_norm(adj_train)\n",
    "adj_label_train = adj_train + I_train\n",
    "\n",
    "adj_norm_test = graph_norm(adj_test)\n",
    "adj_label_test = adj_test + I_test\n",
    "\n",
    "features_test = np.ones((adj_test.shape[0], adj_test.shape[1], 1))\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((adj_norm_train, adj_label_train, features))\n",
    "                    .shuffle(len(adj_norm_train)).batch(64))\n",
    "norm = adj_train.shape[1] * adj_train.shape[1] / float((adj_train.shape[1] * adj_train.shape[1]\n",
    "                                                        - (adj_train.sum() / adj_train.shape[0])) * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Variational Autoencoder\n",
    "class GCNModelVAE(tf.keras.Model):\n",
    "    def __init__(self, num_features, num_nodes, features_nonzero, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.n_samples = num_nodes\n",
    "        self.hidden_dim = 16  # hyperparameter\n",
    "        self.embedding_dimension = 1  # hyperparameter\n",
    "        self.hidden1 = GraphConvolution(input_dim=self.input_dim,\n",
    "                                        output_dim=self.hidden_dim, num=1,\n",
    "                                        act=lambda x: x, trainable=True)  # Convolutional layer\n",
    "        self.hidden2 = GraphConvolution(input_dim=self.hidden_dim,\n",
    "                                        output_dim=self.embedding_dimension * 2, num=2,\n",
    "                                        act=lambda x: x, trainable=True)\n",
    "        self.d = InnerProductDecoder(act=lambda x: x)\n",
    "        self.d1 = GraphConvolution(input_dim=1,\n",
    "                                   output_dim=self.n_samples, num=3,\n",
    "                                   act=lambda x: x, trainable=True)  # Use new decoder model and loss function = 3???\n",
    "    \n",
    "    # Encoder feedforward\n",
    "    def encoder(self, inputs, adj, rate):\n",
    "        x = self.hidden1(inputs, adj=adj, rate=rate)\n",
    "        x = self.hidden2(x, adj=adj, rate=rate)\n",
    "        mean, logvar = tf.split(x, num_or_size_splits=2, axis=2)\n",
    "        return mean, logvar\n",
    "\n",
    "    # Reparametrization trick\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal([self.n_samples, self.embedding_dimension])\n",
    "        return eps * (tf.exp(logvar)) + mean\n",
    "\n",
    "    # Decoding model\n",
    "    def decoder(self, z, adj, rate=0., apply_sigmoid=False):\n",
    "        logits = z\n",
    "        logits = self.d(logits, rate=0.)\n",
    "        feature = tf.ones((logits.shape[0], logits.shape[1], 1))\n",
    "        logits = self.d1(feature, adj=logits, rate=rate)\n",
    "        logits = tf.reshape(logits, [-1, self.n_samples * self.n_samples])\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)  # learning rate\n",
    "\n",
    "\n",
    "# VAE optimizer model\n",
    "class OptimizerVAE(object):\n",
    "    def __init__(self, model, num_nodes, num_features, norm):\n",
    "        self.norm = norm\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    def log_normal_pdf(self, sample, mean, logsd, raxis=[1, 2]):\n",
    "        logvar = 2 * logsd\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        out = tf.reduce_sum(-.5 * (tf.multiply((sample - mean) ** 2., tf.exp(-logvar)) + logvar + log2pi), axis=raxis)\n",
    "        return out\n",
    "\n",
    "    def bernoulli_log_density(self, logit, x):\n",
    "        b = (x * 2) - 1\n",
    "        return - tf.math.log(1 + tf.exp(-tf.multiply(b, logit)))\n",
    "\n",
    "    def loss(self, y, x, adj, rate, model):\n",
    "        mean, logvar = model.encoder(x, adj=adj, rate=rate)\n",
    "        reparam = model.reparameterize(mean, logvar)\n",
    "        reconstruct = model.decoder(reparam, adj, rate)\n",
    "        preds_sub = tf.reshape(reconstruct, [-1, self.num_nodes, self.num_nodes])\n",
    "        logpz = self.log_normal_pdf(reparam, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(reparam, mean, logvar)\n",
    "        logpx_z = tf.reduce_sum(self.bernoulli_log_density(preds_sub, tf.cast(y, tf.float32)), [1, 2])\n",
    "        return -tf.reduce_mean(logpx_z - ((logpz - logqz_x)))\n",
    "\n",
    "    def loss2(self, y, x, adj, rate, model):\n",
    "        mean, logvar = model.encoder(x, adj, rate)\n",
    "        reparam = model.reparameterize(mean, logvar)\n",
    "        reconstruct = model.decoder(reparam, adj, rate)\n",
    "        preds_sub = tf.reshape(reconstruct, [-1, self.num_nodes, self.num_nodes])\n",
    "        cost = self.norm * tf.reduce_mean(tf.reduce_sum(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32),\n",
    "                                                    logits=preds_sub), [1, 2]))\n",
    "        kl = (0.5 / self.num_nodes) * \\\n",
    "             tf.reduce_mean(tf.reduce_sum(1 \\\n",
    "                                          + 2 * logvar \\\n",
    "                                          - tf.square(mean) \\\n",
    "                                          - tf.square(tf.exp(logvar)), [1, 2]))\n",
    "        cost -= kl\n",
    "        return cost\n",
    "\n",
    "    def train_step(self, y, x, adj, rate, model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.loss(y, x, adj=adj, rate=rate, model=model)\n",
    "\n",
    "            for i in model.layers:\n",
    "                print(i.name, i.built, i.trainable_weights)\n",
    "        assert not np.any(np.isnan(cost.numpy()))\n",
    "        print(model.trainable_weights)\n",
    "        print(model.summary())\n",
    "        gradients = tape.gradient(cost, model.trainable_variables)\n",
    "        opt_op = self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 57, 1)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Training...\n",
      "graph_convolution_87 True []\n",
      "graph_convolution_88 True []\n",
      "inner_product_decoder_29 True []\n",
      "graph_convolution_89 True []\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gcn_model_vae_29\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gcn_model_vae_29\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ graph_convolution_87            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GraphConvolution</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ graph_convolution_88            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GraphConvolution</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ inner_product_decoder_29        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InnerProductDecoder</span>)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ graph_convolution_89            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GraphConvolution</span>)              │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ graph_convolution_87            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGraphConvolution\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ graph_convolution_88            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGraphConvolution\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ inner_product_decoder_29        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mInnerProductDecoder\u001b[0m)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ graph_convolution_89            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGraphConvolution\u001b[0m)              │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adj, label, x \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[1;32m---> 21\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGVAE_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m avg_cost \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(adj_train))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(avg_cost, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t, \u001b[38;5;241m3\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[57], line 106\u001b[0m, in \u001b[0;36mOptimizerVAE.train_step\u001b[1;34m(self, y, x, adj, rate, model)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m    105\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(cost, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m--> 106\u001b[0m opt_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "File \u001b[1;32mc:\\Users\\timmy\\.virtualenvs\\RegeneronSTS-MbjA0bbz\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:281\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m--> 281\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "print(\"Initializing...\")\n",
    "# VAE model\n",
    "GVAE_model = GCNModelVAE(num_features, num_nodes, features_nonzero)\n",
    "# Optimizer\n",
    "optimizer = OptimizerVAE(model=GVAE_model, num_nodes=num_nodes,\n",
    "                            num_features=num_features, norm=norm)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "prev_cost = float(\"inf\")\n",
    "stop_val = 0\n",
    "stop_num = 10\n",
    "FLAGS_shuffle = False\n",
    "for epoch in range(1000):\n",
    "    t = time.time()\n",
    "    # Compute average loss\n",
    "    loss = 0\n",
    "    for adj, label, x in train_dataset:\n",
    "        loss += optimizer.train_step(label, \n",
    "                                     tf.cast(x, tf.float32), \n",
    "                                     tf.cast(adj, tf.float32), \n",
    "                                     dropout_rate, GVAE_model)\n",
    "    avg_cost = loss.numpy() / (len(adj_train))\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(round(avg_cost, 3)),\n",
    "            \"time=\", \"{:.5f}\".format(round(time.time() - t, 3)))\n",
    "    Computational_time[run] += (time.time() - t)\n",
    "    num_epoch[run] += 1\n",
    "\n",
    "    # When to stop the iteration\n",
    "    if (prev_cost < avg_cost):\n",
    "        stop_val += 1\n",
    "        if (stop_val == stop_num):\n",
    "            break\n",
    "    else:\n",
    "        stop_val = 0\n",
    "        prev_cost = avg_cost\n",
    "\n",
    "Computational_time[run] = Computational_time[run] / num_epoch[run]\n",
    "\n",
    "print(\"Time for each epoch:\", np.round(Computational_time[run], 3))\n",
    "\n",
    "meanr, logvarr = GVAE_model.encoder(tf.cast(features, tf.float32), tf.cast(adj_norm_train, tf.float32), 0.)\n",
    "ztr = GVAE_model.reparameterize(meanr, logvarr)\n",
    "meane, logvare = GVAE_model.encoder(tf.cast(features_test, tf.float32), tf.cast(adj_norm_test, tf.float32), 0.)\n",
    "zte = GVAE_model.reparameterize(meane, logvare)\n",
    "\n",
    "train_feature = deepcopy(ztr).numpy().reshape(len(ztr), -1)\n",
    "test_feature = deepcopy(zte).numpy().reshape(len(zte), -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RegeneronSTS-MbjA0bbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
