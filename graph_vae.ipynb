{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from graph_clustering import A_binarize, creating_label\n",
    "from graph_features import graph_norm\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "import time\n",
    "from preprocessing_functions import preprocess_data\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that the 3 loss function is defined and we include the new proposed decoder model\n",
    "# Experiment stuff\n",
    "subject_num = 149  # Size of dataset\n",
    "run_num = 1  # Number of \"experiences\" of each task (this should be one)\n",
    "num_electrodes = 60  # Number of EEG electrodes (this is excluding [\"Iz\", \"I1\", \"I2\", \"Resp\", \"PO4\", \"PO3\", \"FT9\", \"Status\"])\n",
    "data_length = 60500  # Minimum data length, cut off anything outside this data length\n",
    "BASE_PATH = r\"C:\\Users\\timmy\\Downloads\\park-eeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the dataset and labels\n",
    "def load_dataset():\n",
    "    # Format is sub-NUM_task-Rest_eeg.edf\n",
    "    data = []\n",
    "    for subject in range(subject_num):\n",
    "        subject_list = []\n",
    "        for run in range(run_num):\n",
    "            file_name = r\"\\sub-{}_task-Rest_eeg.edf\".format(str(subject + 1).zfill(3))\n",
    "            try:\n",
    "                f = pyedflib.EdfReader(BASE_PATH + file_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            electrode_list = []\n",
    "            for electrode in range(num_electrodes):  # Electrodes are zero-indexed\n",
    "                # Iz, I1, I2, PO4, PO3, FT9 is not present in all subjects... exclude\n",
    "                # Status/Resp... I'm assuming is ground/reference electrode\n",
    "                # Regardless, both status/resp are not present in all subjects\n",
    "                if f.getLabel(electrode) in (\"Iz\", \"I1\", \"I2\", \"Resp\", \"PO4\", \"PO3\", \"FT9\", \"Status\"):\n",
    "                    continue\n",
    "                electrode_list.append(f.readSignal(electrode)[:data_length])\n",
    "            subject_list.append(electrode_list)\n",
    "            f._close()\n",
    "            del f  # Don't read all files into memory\n",
    "\n",
    "        data.append(subject_list)\n",
    "        print(\"Subject\", subject, \"done!\")\n",
    "\n",
    "    raw_labels = pd.read_csv(\"participants.tsv\", sep=\"\\t\")\n",
    "    # Get rid of #68 and add labels\n",
    "    new_data = []\n",
    "    labels = []\n",
    "    for index, subject in enumerate(data):\n",
    "\n",
    "        if not subject:\n",
    "            continue\n",
    "\n",
    "        new_data.append(subject)\n",
    "\n",
    "        query = \"sub-\" + str(index + 1).zfill(3)\n",
    "        results = raw_labels[(raw_labels[\"participant_id\"] == query)]\n",
    "        labels.append(results.iloc[0][\"GROUP\"])\n",
    "\n",
    "        print(\"Label\", index, \"done!\")\n",
    "\n",
    "    return np.array(new_data), np.array([i == \"PD\" for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0 done!\n",
      "Subject 1 done!\n",
      "Subject 2 done!\n",
      "Subject 3 done!\n",
      "Subject 4 done!\n",
      "Subject 5 done!\n",
      "Subject 6 done!\n",
      "Subject 7 done!\n",
      "Subject 8 done!\n",
      "Subject 9 done!\n",
      "Subject 10 done!\n",
      "Subject 11 done!\n",
      "Subject 12 done!\n",
      "Subject 13 done!\n",
      "Subject 14 done!\n",
      "Subject 15 done!\n",
      "Subject 16 done!\n",
      "Subject 17 done!\n",
      "Subject 18 done!\n",
      "Subject 19 done!\n",
      "Subject 20 done!\n",
      "Subject 21 done!\n",
      "Subject 22 done!\n",
      "Subject 23 done!\n",
      "Subject 24 done!\n",
      "Subject 25 done!\n",
      "Subject 26 done!\n",
      "Subject 27 done!\n",
      "Subject 28 done!\n",
      "Subject 29 done!\n",
      "Subject 30 done!\n",
      "Subject 31 done!\n",
      "Subject 32 done!\n",
      "Subject 33 done!\n",
      "Subject 34 done!\n",
      "Subject 35 done!\n",
      "Subject 36 done!\n",
      "Subject 37 done!\n",
      "Subject 38 done!\n",
      "Subject 39 done!\n",
      "Subject 40 done!\n",
      "Subject 41 done!\n",
      "Subject 42 done!\n",
      "Subject 43 done!\n",
      "Subject 44 done!\n",
      "Subject 45 done!\n",
      "Subject 46 done!\n",
      "Subject 47 done!\n",
      "Subject 48 done!\n",
      "Subject 49 done!\n",
      "Subject 50 done!\n",
      "Subject 51 done!\n",
      "Subject 52 done!\n",
      "Subject 53 done!\n",
      "Subject 54 done!\n",
      "Subject 55 done!\n",
      "Subject 56 done!\n",
      "Subject 57 done!\n",
      "Subject 58 done!\n",
      "Subject 59 done!\n",
      "Subject 60 done!\n",
      "Subject 61 done!\n",
      "Subject 62 done!\n",
      "Subject 63 done!\n",
      "Subject 64 done!\n",
      "Subject 65 done!\n",
      "Subject 66 done!\n",
      "C:\\Users\\timmy\\Downloads\\park-eeg\\sub-068_task-Rest_eeg.edf: the file is not EDF(+) or BDF(+) compliant (Physical Minimum)\n",
      "Subject 67 done!\n",
      "Subject 68 done!\n",
      "Subject 69 done!\n",
      "Subject 70 done!\n",
      "Subject 71 done!\n",
      "Subject 72 done!\n",
      "Subject 73 done!\n",
      "Subject 74 done!\n",
      "Subject 75 done!\n",
      "Subject 76 done!\n",
      "Subject 77 done!\n",
      "Subject 78 done!\n",
      "Subject 79 done!\n",
      "Subject 80 done!\n",
      "Subject 81 done!\n",
      "Subject 82 done!\n",
      "Subject 83 done!\n",
      "Subject 84 done!\n",
      "Subject 85 done!\n",
      "Subject 86 done!\n",
      "Subject 87 done!\n",
      "Subject 88 done!\n",
      "Subject 89 done!\n",
      "Subject 90 done!\n",
      "Subject 91 done!\n",
      "Subject 92 done!\n",
      "Subject 93 done!\n",
      "Subject 94 done!\n",
      "Subject 95 done!\n",
      "Subject 96 done!\n",
      "Subject 97 done!\n",
      "Subject 98 done!\n",
      "Subject 99 done!\n",
      "Subject 100 done!\n",
      "Subject 101 done!\n",
      "Subject 102 done!\n",
      "Subject 103 done!\n",
      "Subject 104 done!\n",
      "Subject 105 done!\n",
      "Subject 106 done!\n",
      "Subject 107 done!\n",
      "Subject 108 done!\n",
      "Subject 109 done!\n",
      "Subject 110 done!\n",
      "Subject 111 done!\n",
      "Subject 112 done!\n",
      "Subject 113 done!\n",
      "Subject 114 done!\n",
      "Subject 115 done!\n",
      "Subject 116 done!\n",
      "Subject 117 done!\n",
      "Subject 118 done!\n",
      "Subject 119 done!\n",
      "Subject 120 done!\n",
      "Subject 121 done!\n",
      "Subject 122 done!\n",
      "Subject 123 done!\n",
      "Subject 124 done!\n",
      "Subject 125 done!\n",
      "Subject 126 done!\n",
      "Subject 127 done!\n",
      "Subject 128 done!\n",
      "Subject 129 done!\n",
      "Subject 130 done!\n",
      "Subject 131 done!\n",
      "Subject 132 done!\n",
      "Subject 133 done!\n",
      "Subject 134 done!\n",
      "Subject 135 done!\n",
      "Subject 136 done!\n",
      "Subject 137 done!\n",
      "Subject 138 done!\n",
      "Subject 139 done!\n",
      "Subject 140 done!\n",
      "Subject 141 done!\n",
      "Subject 142 done!\n",
      "Subject 143 done!\n",
      "Subject 144 done!\n",
      "Subject 145 done!\n",
      "Subject 146 done!\n",
      "Subject 147 done!\n",
      "Subject 148 done!\n",
      "Label 0 done!\n",
      "Label 1 done!\n",
      "Label 2 done!\n",
      "Label 3 done!\n",
      "Label 4 done!\n",
      "Label 5 done!\n",
      "Label 6 done!\n",
      "Label 7 done!\n",
      "Label 8 done!\n",
      "Label 9 done!\n",
      "Label 10 done!\n",
      "Label 11 done!\n",
      "Label 12 done!\n",
      "Label 13 done!\n",
      "Label 14 done!\n",
      "Label 15 done!\n",
      "Label 16 done!\n",
      "Label 17 done!\n",
      "Label 18 done!\n",
      "Label 19 done!\n",
      "Label 20 done!\n",
      "Label 21 done!\n",
      "Label 22 done!\n",
      "Label 23 done!\n",
      "Label 24 done!\n",
      "Label 25 done!\n",
      "Label 26 done!\n",
      "Label 27 done!\n",
      "Label 28 done!\n",
      "Label 29 done!\n",
      "Label 30 done!\n",
      "Label 31 done!\n",
      "Label 32 done!\n",
      "Label 33 done!\n",
      "Label 34 done!\n",
      "Label 35 done!\n",
      "Label 36 done!\n",
      "Label 37 done!\n",
      "Label 38 done!\n",
      "Label 39 done!\n",
      "Label 40 done!\n",
      "Label 41 done!\n",
      "Label 42 done!\n",
      "Label 43 done!\n",
      "Label 44 done!\n",
      "Label 45 done!\n",
      "Label 46 done!\n",
      "Label 47 done!\n",
      "Label 48 done!\n",
      "Label 49 done!\n",
      "Label 50 done!\n",
      "Label 51 done!\n",
      "Label 52 done!\n",
      "Label 53 done!\n",
      "Label 54 done!\n",
      "Label 55 done!\n",
      "Label 56 done!\n",
      "Label 57 done!\n",
      "Label 58 done!\n",
      "Label 59 done!\n",
      "Label 60 done!\n",
      "Label 61 done!\n",
      "Label 62 done!\n",
      "Label 63 done!\n",
      "Label 64 done!\n",
      "Label 65 done!\n",
      "Label 66 done!\n",
      "Label 68 done!\n",
      "Label 69 done!\n",
      "Label 70 done!\n",
      "Label 71 done!\n",
      "Label 72 done!\n",
      "Label 73 done!\n",
      "Label 74 done!\n",
      "Label 75 done!\n",
      "Label 76 done!\n",
      "Label 77 done!\n",
      "Label 78 done!\n",
      "Label 79 done!\n",
      "Label 80 done!\n",
      "Label 81 done!\n",
      "Label 82 done!\n",
      "Label 83 done!\n",
      "Label 84 done!\n",
      "Label 85 done!\n",
      "Label 86 done!\n",
      "Label 87 done!\n",
      "Label 88 done!\n",
      "Label 89 done!\n",
      "Label 90 done!\n",
      "Label 91 done!\n",
      "Label 92 done!\n",
      "Label 93 done!\n",
      "Label 94 done!\n",
      "Label 95 done!\n",
      "Label 96 done!\n",
      "Label 97 done!\n",
      "Label 98 done!\n",
      "Label 99 done!\n",
      "Label 100 done!\n",
      "Label 101 done!\n",
      "Label 102 done!\n",
      "Label 103 done!\n",
      "Label 104 done!\n",
      "Label 105 done!\n",
      "Label 106 done!\n",
      "Label 107 done!\n",
      "Label 108 done!\n",
      "Label 109 done!\n",
      "Label 110 done!\n",
      "Label 111 done!\n",
      "Label 112 done!\n",
      "Label 113 done!\n",
      "Label 114 done!\n",
      "Label 115 done!\n",
      "Label 116 done!\n",
      "Label 117 done!\n",
      "Label 118 done!\n",
      "Label 119 done!\n",
      "Label 120 done!\n",
      "Label 121 done!\n",
      "Label 122 done!\n",
      "Label 123 done!\n",
      "Label 124 done!\n",
      "Label 125 done!\n",
      "Label 126 done!\n",
      "Label 127 done!\n",
      "Label 128 done!\n",
      "Label 129 done!\n",
      "Label 130 done!\n",
      "Label 131 done!\n",
      "Label 132 done!\n",
      "Label 133 done!\n",
      "Label 134 done!\n",
      "Label 135 done!\n",
      "Label 136 done!\n",
      "Label 137 done!\n",
      "Label 138 done!\n",
      "Label 139 done!\n",
      "Label 140 done!\n",
      "Label 141 done!\n",
      "Label 142 done!\n",
      "Label 143 done!\n",
      "Label 144 done!\n",
      "Label 145 done!\n",
      "Label 146 done!\n",
      "Label 147 done!\n",
      "Label 148 done!\n"
     ]
    }
   ],
   "source": [
    "Binary = True\n",
    "partial_subject = False\n",
    "part_channel = False\n",
    "verbose = True\n",
    "nb_run = 5  # 5-fold cross validation\n",
    "step = 160 * 0 + 80  # 1-window*alpha%\n",
    "Fs = 500  # Sampling frequency (hz)\n",
    "Ts = 1 / Fs  # Time in seconds ??\n",
    "\n",
    "# Initialized numpy arrays for in-train data\n",
    "accuracy = np.zeros((nb_run, 1))\n",
    "accuracy2 = np.zeros((nb_run, 1))\n",
    "Computational_time = np.zeros((nb_run, 1))\n",
    "num_epoch = np.zeros((nb_run, 1))\n",
    "full_time = np.zeros((nb_run, 1))\n",
    "roc_auc = np.zeros((nb_run, 1))\n",
    "EER = np.zeros((nb_run, 1))\n",
    "\n",
    "x_raw_all, labels = load_dataset()  # import data for all subject \n",
    "\n",
    "loss_function = 3  # 3 loss function is defined\n",
    "decoder_adj = True  # include new decoder model\n",
    "\n",
    "FLAGS_features = False  # Whether or not to include pre-defined features\n",
    "features_init_train = None\n",
    "features_init_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invlogit(z):  # convert decoded adjancy matrix to original space\n",
    "    return 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "def Adj_matrix(train_x, test_x):\n",
    "    if (Binary):\n",
    "        # Change weighted matrix to binary matrix with threshold\n",
    "        percentile = 0.75\n",
    "        adj_train = A_binarize(A_matrix=train_x, percent=percentile, sparse=False)\n",
    "        adj_test = A_binarize(A_matrix=test_x, percent=percentile, sparse=False)\n",
    "        # sparse matrix\n",
    "    else:\n",
    "        adj_train = deepcopy(train_x)\n",
    "        adj_test = deepcopy(test_x)\n",
    "    # consider part of the graph\n",
    "    print(\"sparsity: \", scipy.sparse.issparse(adj_train[9]))  # check sparsity\n",
    "    print(\"rank: \", np.linalg.matrix_rank(adj_train[9]))  # check matrix rank\n",
    "    return adj_train, adj_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA...\n",
      "Start ICA\n",
      "END ICA\n",
      "ADF Statistic: -54.916290\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.431\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n",
      "Creating brain graph....\n",
      "sparsity:  False\n",
      "rank:  57\n",
      "Preprocessing...\n"
     ]
    }
   ],
   "source": [
    "run = 0\n",
    "dropout_rate = 0.5\n",
    "t_start = time.time()\n",
    "\n",
    "print(\"Applying ICA...\")\n",
    "sec = 12  # Number of seconds in window?\n",
    "\n",
    "train_x, test_x, y_train, y_test = preprocess_data(x_raw_all[:, 0], labels, run, Fs,\n",
    "                                                    filt=False, ICA=True, A_Matrix='cov', sec=sec)\n",
    "\n",
    "print(\"Creating brain graph....\")\n",
    "adj_train, adj_test = Adj_matrix(train_x, test_x)  # Creating functional connectivity graph\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "# Compute number of nodes\n",
    "num_nodes = adj_train.shape[1]\n",
    "\n",
    "# If features are not used, replace feature matrix by identity matrix\n",
    "I_train = np.tile(np.eye(adj_train.shape[1]), adj_train.shape[0]).T.reshape(-1, adj_train.shape[1],\n",
    "                                                                            adj_train.shape[1])\n",
    "I_test = np.tile(np.eye(adj_test.shape[1]), adj_test.shape[0]).T.reshape(-1, adj_test.shape[1], adj_test.shape[1])\n",
    "\n",
    "features_train = np.ones((adj_train.shape[0], adj_train.shape[1], 1))\n",
    "\n",
    "# Preprocessing on node features\n",
    "num_features = features_train.shape[2]\n",
    "features_nonzero = np.count_nonzero(features_train) // features_train.shape[0]\n",
    "\n",
    "# Normalization and preprocessing on adjacency matrix\n",
    "adj_norm_train = graph_norm(adj_train)\n",
    "adj_label_train = adj_train + I_train\n",
    "\n",
    "adj_norm_test = graph_norm(adj_test)\n",
    "adj_label_test = adj_test + I_test\n",
    "\n",
    "features_test = np.ones((adj_test.shape[0], adj_test.shape[1], 1))\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((adj_norm_train, adj_label_train, features_train))\n",
    "                    .shuffle(len(adj_norm_train)).batch(64))\n",
    "norm = adj_train.shape[1] * adj_train.shape[1] / float((adj_train.shape[1] * adj_train.shape[1]\n",
    "                                                        - (adj_train.sum() / adj_train.shape[0])) * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 57 57 0.666871921182266\n"
     ]
    }
   ],
   "source": [
    "print(num_features, num_nodes, features_nonzero, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type = \"float32\"\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    \"\"\" Graph convolution layer \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num, act=tf.nn.relu, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = self.add_weight(name='weight' + str(num), shape=(input_dim, output_dim),\n",
    "                             initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(name='bias' + str(num),\n",
    "                             initializer=\"zeros\", shape=(output_dim,), trainable=True)\n",
    "        self.act = act\n",
    "\n",
    "\n",
    "    def call(self, inputs, adj, rate=0., normalize=False):\n",
    "        x = tf.nn.dropout(inputs, rate=rate)\n",
    "        x = tf.matmul(x, self.w)\n",
    "        x = tf.matmul(adj, x)\n",
    "        outputs = self.act(x + self.b)\n",
    "        if normalize:\n",
    "            x = tf.keras.utils.normalize(x)\n",
    "        return outputs\n",
    "\n",
    "class InnerProductDecoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Symmetric inner product decoder layer\"\"\"\n",
    "\n",
    "    def __init__(self, act=tf.nn.sigmoid, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.act = act\n",
    "\n",
    "    def call(self, inputs, rate=0.):\n",
    "        inputs = tf.nn.dropout(inputs, rate=rate)\n",
    "        if (tf.shape(inputs).shape == 3):\n",
    "            x = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        else:\n",
    "            x = tf.transpose(inputs)\n",
    "        x = tf.matmul(inputs, x)\n",
    "        outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "# Graph Variational Autoencoder\n",
    "class GCNModelVAE(tf.keras.Model):\n",
    "    def __init__(self, num_features, num_nodes, features_nonzero, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.n_samples = num_nodes\n",
    "        self.hidden_dim = 16  # hyperparameter\n",
    "        self.embedding_dimension = 1  # hyperparameter\n",
    "        self.hidden1 = GraphConvolution(input_dim=self.input_dim,\n",
    "                                        output_dim=self.hidden_dim, num=1,\n",
    "                                        act=lambda x: x, trainable=True)  # Convolutional layer\n",
    "        self.hidden2 = GraphConvolution(input_dim=self.hidden_dim,\n",
    "                                        output_dim=self.embedding_dimension * 2, num=2,\n",
    "                                        act=lambda x: x, trainable=True)\n",
    "        self.d = InnerProductDecoder(act=lambda x: x)\n",
    "        self.d1 = GraphConvolution(input_dim=1,\n",
    "                                   output_dim=self.n_samples, num=3,\n",
    "                                   act=lambda x: x, trainable=True)  # Use new decoder model and loss function = 3???\n",
    "    \n",
    "    # Encoder feedforward\n",
    "    def encoder(self, inputs, adj, rate):\n",
    "        x = self.hidden1(inputs, adj=adj, rate=rate)\n",
    "        x = self.hidden2(x, adj=adj, rate=rate)\n",
    "        mean, logvar = tf.split(x, num_or_size_splits=2, axis=2)\n",
    "        return mean, logvar\n",
    "\n",
    "    # Reparametrization trick\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal([self.n_samples, self.embedding_dimension])\n",
    "        return eps * (tf.exp(logvar)) + mean\n",
    "\n",
    "    # Decoding model\n",
    "    def decoder(self, z, adj, rate=0., apply_sigmoid=False):\n",
    "        logits = z\n",
    "        logits = self.d(logits, rate=0.)\n",
    "        feature = tf.ones((logits.shape[0], logits.shape[1], 1))\n",
    "        logits = self.d1(feature, adj=logits, rate=rate)\n",
    "        logits = tf.reshape(logits, [-1, self.n_samples * self.n_samples])\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)  # learning rate\n",
    "\n",
    "\n",
    "# VAE optimizer model\n",
    "class OptimizerVAE(object):\n",
    "    def __init__(self, model, num_nodes, num_features, norm):\n",
    "        self.norm = norm\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    def log_normal_pdf(self, sample, mean, logsd, raxis=[1, 2]):\n",
    "        logvar = 2 * logsd\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        out = tf.reduce_sum(-.5 * (tf.multiply((sample - mean) ** 2., tf.exp(-logvar)) + logvar + log2pi), axis=raxis)\n",
    "        return out\n",
    "\n",
    "    def bernoulli_log_density(self, logit, x):\n",
    "        b = (x * 2) - 1\n",
    "        return - tf.math.log(1 + tf.exp(-tf.multiply(b, logit)))\n",
    "\n",
    "    def loss(self, y, x, adj, rate, model):\n",
    "        mean, logvar = model.encoder(x, adj=adj, rate=rate)\n",
    "        reparam = model.reparameterize(mean, logvar)\n",
    "        reconstruct = model.decoder(reparam, adj, rate)\n",
    "\n",
    "        \n",
    "        preds_sub = tf.reshape(reconstruct, [-1, self.num_nodes, self.num_nodes])\n",
    "        logpz = self.log_normal_pdf(reparam, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(reparam, mean, logvar)\n",
    "        logpx_z = tf.reduce_sum(self.bernoulli_log_density(preds_sub, tf.cast(y, tf.float32)), [1, 2])\n",
    "        return -tf.reduce_mean(logpx_z - ((logpz - logqz_x)))\n",
    "\n",
    "    def loss2(self, y, x, adj, rate, model):\n",
    "        mean, logvar = model.encoder(x, adj, rate)\n",
    "        reparam = model.reparameterize(mean, logvar)\n",
    "        reconstruct = model.decoder(reparam, adj, rate)\n",
    "        preds_sub = tf.reshape(reconstruct, [-1, self.num_nodes, self.num_nodes])\n",
    "        cost = self.norm * tf.reduce_mean(tf.reduce_sum(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32),\n",
    "                                                    logits=preds_sub), [1, 2]))\n",
    "        kl = (0.5 / self.num_nodes) * \\\n",
    "             tf.reduce_mean(tf.reduce_sum(1 \\\n",
    "                                          + 2 * logvar \\\n",
    "                                          - tf.square(mean) \\\n",
    "                                          - tf.square(tf.exp(logvar)), [1, 2]))\n",
    "        cost -= kl\n",
    "        return cost\n",
    "\n",
    "    def train_step(self, y, x, adj, rate, model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.loss(y, x, adj=adj, rate=rate, model=model)\n",
    "        assert not np.any(np.isnan(cost.numpy()))\n",
    "        gradients = tape.gradient(cost, model.trainable_variables)\n",
    "        opt_op = self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing...\")\n",
    "# VAE model\n",
    "GVAE_model = GCNModelVAE(num_features, num_nodes, features_nonzero)\n",
    "# Optimizer\n",
    "optimizer = OptimizerVAE(model=GVAE_model, num_nodes=num_nodes,\n",
    "                            num_features=num_features, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(1, 16), dtype=float32, path=graph_convolution_22/weight1>,\n",
       " <KerasVariable shape=(16,), dtype=float32, path=graph_convolution_22/bias1>,\n",
       " <KerasVariable shape=(16, 2), dtype=float32, path=graph_convolution_23/weight2>,\n",
       " <KerasVariable shape=(2,), dtype=float32, path=graph_convolution_23/bias2>,\n",
       " <KerasVariable shape=(1, 57), dtype=float32, path=graph_convolution_24/weight3>,\n",
       " <KerasVariable shape=(57,), dtype=float32, path=graph_convolution_24/bias3>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GVAE_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 37.90500 time= 0.72000\n",
      "Epoch: 0002 train_loss= 37.08800 time= 0.70700\n",
      "Epoch: 0003 train_loss= 36.96200 time= 0.71800\n",
      "Epoch: 0004 train_loss= 37.10700 time= 0.70100\n",
      "Epoch: 0005 train_loss= 37.68900 time= 0.69700\n",
      "Epoch: 0006 train_loss= 37.13800 time= 0.68100\n",
      "Epoch: 0007 train_loss= 36.94400 time= 0.67900\n",
      "Epoch: 0008 train_loss= 36.84300 time= 0.69300\n",
      "Epoch: 0009 train_loss= 36.55700 time= 0.68100\n",
      "Epoch: 0010 train_loss= 36.69500 time= 0.68400\n",
      "Epoch: 0011 train_loss= 36.58500 time= 0.71600\n",
      "Epoch: 0012 train_loss= 36.53100 time= 0.70400\n",
      "Epoch: 0013 train_loss= 36.46200 time= 0.69300\n",
      "Epoch: 0014 train_loss= 36.36700 time= 0.69900\n",
      "Epoch: 0015 train_loss= 36.31200 time= 0.68000\n",
      "Epoch: 0016 train_loss= 36.39300 time= 0.69600\n",
      "Epoch: 0017 train_loss= 36.10600 time= 0.69100\n",
      "Epoch: 0018 train_loss= 36.13200 time= 0.69400\n",
      "Epoch: 0019 train_loss= 35.98600 time= 0.67200\n",
      "Epoch: 0020 train_loss= 36.04000 time= 0.67200\n",
      "Epoch: 0021 train_loss= 35.90700 time= 0.67000\n",
      "Epoch: 0022 train_loss= 35.89400 time= 0.66700\n",
      "Epoch: 0023 train_loss= 35.99400 time= 0.67400\n",
      "Epoch: 0024 train_loss= 35.90600 time= 0.67600\n",
      "Epoch: 0025 train_loss= 35.82000 time= 0.68300\n",
      "Epoch: 0026 train_loss= 35.85900 time= 0.67700\n",
      "Epoch: 0027 train_loss= 35.74300 time= 0.67900\n",
      "Epoch: 0028 train_loss= 35.86400 time= 0.68200\n",
      "Epoch: 0029 train_loss= 35.73100 time= 0.66900\n",
      "Epoch: 0030 train_loss= 35.61800 time= 0.68300\n",
      "Epoch: 0031 train_loss= 35.65700 time= 0.67900\n",
      "Epoch: 0032 train_loss= 35.58200 time= 0.69400\n",
      "Epoch: 0033 train_loss= 35.67500 time= 0.72600\n",
      "Epoch: 0034 train_loss= 35.55200 time= 0.68600\n",
      "Epoch: 0035 train_loss= 35.53700 time= 0.67300\n",
      "Epoch: 0036 train_loss= 35.47600 time= 0.68300\n",
      "Epoch: 0037 train_loss= 35.46000 time= 0.68200\n",
      "Epoch: 0038 train_loss= 35.44000 time= 0.66700\n",
      "Epoch: 0039 train_loss= 35.38800 time= 0.67300\n",
      "Epoch: 0040 train_loss= 35.38200 time= 0.69600\n",
      "Epoch: 0041 train_loss= 35.35400 time= 0.67200\n",
      "Epoch: 0042 train_loss= 35.33800 time= 0.67700\n",
      "Epoch: 0043 train_loss= 35.27500 time= 0.67100\n",
      "Epoch: 0044 train_loss= 35.27100 time= 0.66900\n",
      "Epoch: 0045 train_loss= 35.23700 time= 0.67100\n",
      "Epoch: 0046 train_loss= 35.20600 time= 0.67300\n",
      "Epoch: 0047 train_loss= 35.18400 time= 0.69500\n",
      "Epoch: 0048 train_loss= 35.17700 time= 0.69300\n",
      "Epoch: 0049 train_loss= 35.13500 time= 0.67100\n",
      "Epoch: 0050 train_loss= 35.11600 time= 0.67700\n",
      "Epoch: 0051 train_loss= 35.09500 time= 0.69000\n",
      "Epoch: 0052 train_loss= 35.07400 time= 0.67300\n",
      "Epoch: 0053 train_loss= 35.05400 time= 0.68300\n",
      "Epoch: 0054 train_loss= 35.03500 time= 0.68900\n",
      "Epoch: 0055 train_loss= 34.99800 time= 0.69600\n",
      "Epoch: 0056 train_loss= 34.97300 time= 0.68200\n",
      "Epoch: 0057 train_loss= 34.95700 time= 0.69200\n",
      "Epoch: 0058 train_loss= 34.94500 time= 0.67000\n",
      "Epoch: 0059 train_loss= 34.90300 time= 0.67600\n",
      "Epoch: 0060 train_loss= 34.90300 time= 0.67400\n",
      "Epoch: 0061 train_loss= 34.87000 time= 0.67000\n",
      "Epoch: 0062 train_loss= 34.83700 time= 0.69500\n",
      "Epoch: 0063 train_loss= 34.81800 time= 0.67200\n",
      "Epoch: 0064 train_loss= 34.80200 time= 0.67900\n",
      "Epoch: 0065 train_loss= 34.78700 time= 0.68000\n",
      "Epoch: 0066 train_loss= 34.76000 time= 0.68000\n",
      "Epoch: 0067 train_loss= 34.73600 time= 0.68900\n",
      "Epoch: 0068 train_loss= 34.73200 time= 0.69300\n",
      "Epoch: 0069 train_loss= 34.70700 time= 0.68300\n",
      "Epoch: 0070 train_loss= 34.69300 time= 0.68200\n",
      "Epoch: 0071 train_loss= 34.65600 time= 0.68800\n",
      "Epoch: 0072 train_loss= 34.64500 time= 0.67400\n",
      "Epoch: 0073 train_loss= 34.64200 time= 0.68000\n",
      "Epoch: 0074 train_loss= 34.61300 time= 0.68400\n",
      "Epoch: 0075 train_loss= 34.56100 time= 0.67500\n",
      "Epoch: 0076 train_loss= 34.57800 time= 0.67500\n",
      "Epoch: 0077 train_loss= 34.55600 time= 0.70900\n",
      "Epoch: 0078 train_loss= 34.54100 time= 0.66800\n",
      "Epoch: 0079 train_loss= 34.54100 time= 0.67500\n",
      "Epoch: 0080 train_loss= 34.46900 time= 0.66400\n",
      "Epoch: 0081 train_loss= 34.49100 time= 0.67600\n",
      "Epoch: 0082 train_loss= 34.47200 time= 0.67800\n",
      "Epoch: 0083 train_loss= 34.44200 time= 0.68200\n",
      "Epoch: 0084 train_loss= 34.42900 time= 0.68900\n",
      "Epoch: 0085 train_loss= 34.39400 time= 0.67500\n",
      "Epoch: 0086 train_loss= 34.38400 time= 0.67200\n",
      "Epoch: 0087 train_loss= 34.37600 time= 0.67800\n",
      "Epoch: 0088 train_loss= 34.34900 time= 0.70000\n",
      "Epoch: 0089 train_loss= 34.31400 time= 0.67000\n",
      "Epoch: 0090 train_loss= 34.32100 time= 0.67300\n",
      "Epoch: 0091 train_loss= 34.31900 time= 0.70000\n",
      "Epoch: 0092 train_loss= 34.28200 time= 0.67100\n",
      "Epoch: 0093 train_loss= 34.26700 time= 0.67700\n",
      "Epoch: 0094 train_loss= 34.23700 time= 0.67900\n",
      "Epoch: 0095 train_loss= 34.24000 time= 0.67400\n",
      "Epoch: 0096 train_loss= 34.18900 time= 0.67900\n",
      "Epoch: 0097 train_loss= 34.17200 time= 0.68200\n",
      "Epoch: 0098 train_loss= 34.18400 time= 0.67900\n",
      "Epoch: 0099 train_loss= 34.11300 time= 0.70100\n",
      "Epoch: 0100 train_loss= 34.12900 time= 0.68700\n",
      "Epoch: 0101 train_loss= 34.09700 time= 0.67500\n",
      "Epoch: 0102 train_loss= 34.07700 time= 0.67000\n",
      "Epoch: 0103 train_loss= 34.02100 time= 0.66700\n",
      "Epoch: 0104 train_loss= 34.03500 time= 0.67500\n",
      "Epoch: 0105 train_loss= 34.01500 time= 0.67300\n",
      "Epoch: 0106 train_loss= 33.99700 time= 0.70400\n",
      "Epoch: 0107 train_loss= 33.94600 time= 0.67800\n",
      "Epoch: 0108 train_loss= 33.94600 time= 0.66800\n",
      "Epoch: 0109 train_loss= 33.91600 time= 0.67600\n",
      "Epoch: 0110 train_loss= 33.89400 time= 0.68100\n",
      "Epoch: 0111 train_loss= 33.84200 time= 0.68800\n",
      "Epoch: 0112 train_loss= 33.83600 time= 0.68400\n",
      "Epoch: 0113 train_loss= 33.79900 time= 0.69100\n",
      "Epoch: 0114 train_loss= 33.76800 time= 0.68700\n",
      "Epoch: 0115 train_loss= 33.73700 time= 0.67500\n",
      "Epoch: 0116 train_loss= 33.70100 time= 0.68700\n",
      "Epoch: 0117 train_loss= 33.68700 time= 0.69200\n",
      "Epoch: 0118 train_loss= 33.60400 time= 0.68000\n",
      "Epoch: 0119 train_loss= 33.59600 time= 0.68300\n",
      "Epoch: 0120 train_loss= 33.52200 time= 0.68600\n",
      "Epoch: 0121 train_loss= 33.51600 time= 0.69500\n",
      "Epoch: 0122 train_loss= 33.43800 time= 0.68000\n",
      "Epoch: 0123 train_loss= 33.37300 time= 0.66700\n",
      "Epoch: 0124 train_loss= 33.28700 time= 0.67400\n",
      "Epoch: 0125 train_loss= 33.11200 time= 0.67900\n",
      "Epoch: 0126 train_loss= 32.96800 time= 0.67600\n",
      "Epoch: 0127 train_loss= 32.62600 time= 0.67300\n",
      "Epoch: 0128 train_loss= 32.48800 time= 0.69400\n",
      "Epoch: 0129 train_loss= 32.05000 time= 0.67700\n",
      "Epoch: 0130 train_loss= 31.83200 time= 0.68100\n",
      "Epoch: 0131 train_loss= 31.42800 time= 0.68900\n",
      "Epoch: 0132 train_loss= 31.25200 time= 0.67600\n",
      "Epoch: 0133 train_loss= 30.94900 time= 0.68000\n",
      "Epoch: 0134 train_loss= 30.87500 time= 0.69400\n",
      "Epoch: 0135 train_loss= 30.80800 time= 0.70000\n",
      "Epoch: 0136 train_loss= 30.58300 time= 0.69300\n",
      "Epoch: 0137 train_loss= 30.55400 time= 0.69000\n",
      "Epoch: 0138 train_loss= 30.47700 time= 0.67100\n",
      "Epoch: 0139 train_loss= 30.36700 time= 0.68100\n",
      "Epoch: 0140 train_loss= 30.36200 time= 0.71500\n",
      "Epoch: 0141 train_loss= 30.28600 time= 0.68400\n",
      "Epoch: 0142 train_loss= 30.18300 time= 0.68400\n",
      "Epoch: 0143 train_loss= 30.12000 time= 0.70200\n",
      "Epoch: 0144 train_loss= 30.08000 time= 0.67600\n",
      "Epoch: 0145 train_loss= 30.00600 time= 0.68300\n",
      "Epoch: 0146 train_loss= 29.97900 time= 0.68100\n",
      "Epoch: 0147 train_loss= 29.87600 time= 0.70400\n",
      "Epoch: 0148 train_loss= 29.89400 time= 0.69400\n",
      "Epoch: 0149 train_loss= 29.78600 time= 0.69100\n",
      "Epoch: 0150 train_loss= 29.79400 time= 0.71800\n",
      "Epoch: 0151 train_loss= 29.73700 time= 0.72800\n",
      "Epoch: 0152 train_loss= 29.63400 time= 0.70600\n",
      "Epoch: 0153 train_loss= 29.59800 time= 0.69200\n",
      "Epoch: 0154 train_loss= 29.57700 time= 0.69800\n",
      "Epoch: 0155 train_loss= 29.52000 time= 0.69400\n",
      "Epoch: 0156 train_loss= 29.43400 time= 0.68800\n",
      "Epoch: 0157 train_loss= 29.39000 time= 0.71000\n",
      "Epoch: 0158 train_loss= 29.40700 time= 0.67900\n",
      "Epoch: 0159 train_loss= 29.27600 time= 0.69500\n",
      "Epoch: 0160 train_loss= 29.29100 time= 0.69800\n",
      "Epoch: 0161 train_loss= 29.18800 time= 0.68300\n",
      "Epoch: 0162 train_loss= 29.14600 time= 0.68500\n",
      "Epoch: 0163 train_loss= 29.13900 time= 0.69400\n",
      "Epoch: 0164 train_loss= 29.02600 time= 0.69600\n",
      "Epoch: 0165 train_loss= 29.05300 time= 0.67700\n",
      "Epoch: 0166 train_loss= 28.94100 time= 0.67800\n",
      "Epoch: 0167 train_loss= 28.90600 time= 0.67300\n",
      "Epoch: 0168 train_loss= 28.81200 time= 0.67100\n",
      "Epoch: 0169 train_loss= 28.76400 time= 0.68200\n",
      "Epoch: 0170 train_loss= 28.71100 time= 0.67300\n",
      "Epoch: 0171 train_loss= 28.64600 time= 0.68100\n",
      "Epoch: 0172 train_loss= 28.56700 time= 0.69400\n",
      "Epoch: 0173 train_loss= 28.48100 time= 0.68200\n",
      "Epoch: 0174 train_loss= 28.39300 time= 0.68600\n",
      "Epoch: 0175 train_loss= 28.35800 time= 0.68100\n",
      "Epoch: 0176 train_loss= 28.28000 time= 0.68400\n",
      "Epoch: 0177 train_loss= 28.15800 time= 0.68700\n",
      "Epoch: 0178 train_loss= 28.09500 time= 0.68000\n",
      "Epoch: 0179 train_loss= 28.00500 time= 0.70500\n",
      "Epoch: 0180 train_loss= 27.90900 time= 0.68800\n",
      "Epoch: 0181 train_loss= 27.81400 time= 0.67700\n",
      "Epoch: 0182 train_loss= 27.74200 time= 0.68400\n",
      "Epoch: 0183 train_loss= 27.64400 time= 0.68700\n",
      "Epoch: 0184 train_loss= 27.51100 time= 0.67700\n",
      "Epoch: 0185 train_loss= 27.45000 time= 0.68000\n",
      "Epoch: 0186 train_loss= 27.35000 time= 0.69300\n",
      "Epoch: 0187 train_loss= 27.20900 time= 0.67600\n",
      "Epoch: 0188 train_loss= 27.11000 time= 0.67400\n",
      "Epoch: 0189 train_loss= 26.95200 time= 0.68100\n",
      "Epoch: 0190 train_loss= 26.82700 time= 0.67100\n",
      "Epoch: 0191 train_loss= 26.72600 time= 0.67600\n",
      "Epoch: 0192 train_loss= 26.56000 time= 0.67700\n",
      "Epoch: 0193 train_loss= 26.46400 time= 0.67900\n",
      "Epoch: 0194 train_loss= 26.28500 time= 0.70500\n",
      "Epoch: 0195 train_loss= 26.11100 time= 0.67800\n",
      "Epoch: 0196 train_loss= 25.97500 time= 0.68000\n",
      "Epoch: 0197 train_loss= 25.79500 time= 0.68600\n",
      "Epoch: 0198 train_loss= 25.67600 time= 0.67600\n",
      "Epoch: 0199 train_loss= 25.46900 time= 0.67700\n",
      "Epoch: 0200 train_loss= 25.25100 time= 0.68000\n",
      "Epoch: 0201 train_loss= 25.07100 time= 0.69100\n",
      "Epoch: 0202 train_loss= 24.90000 time= 0.67800\n",
      "Epoch: 0203 train_loss= 24.73300 time= 0.68800\n",
      "Epoch: 0204 train_loss= 24.53700 time= 0.67700\n",
      "Epoch: 0205 train_loss= 24.22900 time= 0.68100\n",
      "Epoch: 0206 train_loss= 24.08800 time= 0.69800\n",
      "Epoch: 0207 train_loss= 23.82700 time= 0.67400\n",
      "Epoch: 0208 train_loss= 23.62600 time= 0.69000\n",
      "Epoch: 0209 train_loss= 23.41600 time= 0.68000\n",
      "Epoch: 0210 train_loss= 23.11100 time= 0.68100\n",
      "Epoch: 0211 train_loss= 22.89100 time= 0.67200\n",
      "Epoch: 0212 train_loss= 22.62100 time= 0.67200\n",
      "Epoch: 0213 train_loss= 22.39600 time= 0.67000\n",
      "Epoch: 0214 train_loss= 22.17800 time= 0.68400\n",
      "Epoch: 0215 train_loss= 21.74600 time= 0.68100\n",
      "Epoch: 0216 train_loss= 21.48500 time= 0.69700\n",
      "Epoch: 0217 train_loss= 21.24800 time= 0.68400\n",
      "Epoch: 0218 train_loss= 20.97900 time= 0.67900\n",
      "Epoch: 0219 train_loss= 20.62900 time= 0.67600\n",
      "Epoch: 0220 train_loss= 20.31700 time= 0.68000\n",
      "Epoch: 0221 train_loss= 19.98700 time= 0.68000\n",
      "Epoch: 0222 train_loss= 19.61400 time= 0.67900\n",
      "Epoch: 0223 train_loss= 19.37600 time= 0.70600\n",
      "Epoch: 0224 train_loss= 19.09400 time= 0.67800\n",
      "Epoch: 0225 train_loss= 18.63900 time= 0.67600\n",
      "Epoch: 0226 train_loss= 18.27300 time= 0.68200\n",
      "Epoch: 0227 train_loss= 17.85000 time= 0.66900\n",
      "Epoch: 0228 train_loss= 17.52100 time= 0.67600\n",
      "Epoch: 0229 train_loss= 17.11000 time= 0.67000\n",
      "Epoch: 0230 train_loss= 16.71600 time= 0.68800\n",
      "Epoch: 0231 train_loss= 16.20500 time= 0.68300\n",
      "Epoch: 0232 train_loss= 15.98000 time= 0.67000\n",
      "Epoch: 0233 train_loss= 15.60400 time= 0.67500\n",
      "Epoch: 0234 train_loss= 14.96700 time= 0.67700\n",
      "Epoch: 0235 train_loss= 14.63200 time= 0.67100\n",
      "Epoch: 0236 train_loss= 14.31200 time= 0.67700\n",
      "Epoch: 0237 train_loss= 13.68100 time= 0.68700\n",
      "Epoch: 0238 train_loss= 13.31900 time= 0.69900\n",
      "Epoch: 0239 train_loss= 12.86000 time= 0.68200\n",
      "Epoch: 0240 train_loss= 12.36500 time= 0.68600\n",
      "Epoch: 0241 train_loss= 11.99500 time= 0.67400\n",
      "Epoch: 0242 train_loss= 11.48100 time= 0.67900\n",
      "Epoch: 0243 train_loss= 10.91200 time= 0.69800\n",
      "Epoch: 0244 train_loss= 10.39800 time= 0.67000\n",
      "Epoch: 0245 train_loss= 9.72800 time= 0.70700\n",
      "Epoch: 0246 train_loss= 9.22600 time= 0.68700\n",
      "Epoch: 0247 train_loss= 8.79200 time= 0.67700\n",
      "Epoch: 0248 train_loss= 8.03100 time= 0.67700\n",
      "Epoch: 0249 train_loss= 7.58200 time= 0.67700\n",
      "Epoch: 0250 train_loss= 7.03600 time= 0.67600\n",
      "Epoch: 0251 train_loss= 6.54800 time= 0.67300\n",
      "Epoch: 0252 train_loss= 5.93400 time= 0.69600\n",
      "Epoch: 0253 train_loss= 5.32800 time= 0.67700\n",
      "Epoch: 0254 train_loss= 4.67100 time= 0.67400\n",
      "Epoch: 0255 train_loss= 3.99000 time= 0.67600\n",
      "Epoch: 0256 train_loss= 3.37500 time= 0.68100\n",
      "Epoch: 0257 train_loss= 2.83800 time= 0.69200\n",
      "Epoch: 0258 train_loss= 2.02400 time= 0.68000\n",
      "Epoch: 0259 train_loss= 1.30400 time= 0.68700\n",
      "Epoch: 0260 train_loss= 0.72000 time= 0.70900\n",
      "Epoch: 0261 train_loss= -0.08100 time= 0.68400\n",
      "Epoch: 0262 train_loss= -0.70100 time= 0.67900\n",
      "Epoch: 0263 train_loss= -1.21800 time= 0.68700\n",
      "Epoch: 0264 train_loss= -1.90600 time= 0.67900\n",
      "Epoch: 0265 train_loss= -2.87800 time= 0.68700\n",
      "Epoch: 0266 train_loss= -3.55500 time= 0.68400\n",
      "Epoch: 0267 train_loss= -4.22700 time= 0.68800\n",
      "Epoch: 0268 train_loss= -5.09900 time= 0.67000\n",
      "Epoch: 0269 train_loss= -5.89800 time= 0.68200\n",
      "Epoch: 0270 train_loss= -6.81800 time= 0.68200\n",
      "Epoch: 0271 train_loss= -7.54400 time= 0.67200\n",
      "Epoch: 0272 train_loss= -8.18100 time= 0.67200\n",
      "Epoch: 0273 train_loss= -9.04500 time= 0.67900\n",
      "Epoch: 0274 train_loss= -9.67500 time= 0.69300\n",
      "Epoch: 0275 train_loss= -10.74400 time= 0.66900\n",
      "Epoch: 0276 train_loss= -11.36300 time= 0.67900\n",
      "Epoch: 0277 train_loss= -12.70500 time= 0.67300\n",
      "Epoch: 0278 train_loss= -13.33700 time= 0.67500\n",
      "Epoch: 0279 train_loss= -13.90400 time= 0.67700\n",
      "Epoch: 0280 train_loss= -15.05800 time= 0.69100\n",
      "Epoch: 0281 train_loss= -16.11800 time= 0.67900\n",
      "Epoch: 0282 train_loss= -16.54400 time= 0.70400\n",
      "Epoch: 0283 train_loss= -17.97400 time= 0.68600\n",
      "Epoch: 0284 train_loss= -18.94700 time= 0.66800\n",
      "Epoch: 0285 train_loss= -20.20000 time= 0.67900\n",
      "Epoch: 0286 train_loss= -20.52000 time= 0.68900\n",
      "Epoch: 0287 train_loss= -21.78900 time= 0.67900\n",
      "Epoch: 0288 train_loss= -22.88400 time= 0.68100\n",
      "Epoch: 0289 train_loss= -23.79500 time= 0.72000\n",
      "Epoch: 0290 train_loss= -25.08800 time= 0.67700\n",
      "Epoch: 0291 train_loss= -25.75500 time= 0.67900\n",
      "Epoch: 0292 train_loss= -26.77300 time= 0.68700\n",
      "Epoch: 0293 train_loss= -27.84700 time= 0.67500\n",
      "Epoch: 0294 train_loss= -29.21200 time= 0.66500\n",
      "Epoch: 0295 train_loss= -29.74100 time= 0.67700\n",
      "Epoch: 0296 train_loss= -31.12000 time= 0.69100\n",
      "Epoch: 0297 train_loss= -31.97800 time= 0.67600\n",
      "Epoch: 0298 train_loss= -33.35200 time= 0.68100\n",
      "Epoch: 0299 train_loss= -34.34900 time= 0.68100\n",
      "Epoch: 0300 train_loss= -35.17100 time= 0.69300\n",
      "Epoch: 0301 train_loss= -36.70400 time= 0.67300\n",
      "Epoch: 0302 train_loss= -37.72700 time= 0.67500\n",
      "Epoch: 0303 train_loss= -39.07700 time= 0.68900\n",
      "Epoch: 0304 train_loss= -39.84500 time= 0.69400\n",
      "Epoch: 0305 train_loss= -41.08400 time= 0.68400\n",
      "Epoch: 0306 train_loss= -42.30800 time= 0.68900\n",
      "Epoch: 0307 train_loss= -44.10800 time= 0.67900\n",
      "Epoch: 0308 train_loss= -44.87400 time= 0.68700\n",
      "Epoch: 0309 train_loss= -46.08200 time= 0.68400\n",
      "Epoch: 0310 train_loss= -47.89200 time= 0.67800\n",
      "Epoch: 0311 train_loss= -48.53600 time= 0.72100\n",
      "Epoch: 0312 train_loss= -49.57100 time= 0.69000\n",
      "Epoch: 0313 train_loss= -51.63500 time= 0.68100\n",
      "Epoch: 0314 train_loss= -52.20300 time= 0.67300\n",
      "Epoch: 0315 train_loss= -53.71000 time= 0.68100\n",
      "Epoch: 0316 train_loss= -55.06200 time= 0.67700\n",
      "Epoch: 0317 train_loss= -56.60300 time= 0.68000\n",
      "Epoch: 0318 train_loss= -58.02700 time= 0.68800\n",
      "Epoch: 0319 train_loss= -59.32300 time= 0.68800\n",
      "Epoch: 0320 train_loss= -61.65700 time= 0.70900\n",
      "Epoch: 0321 train_loss= -62.07600 time= 0.67600\n",
      "Epoch: 0322 train_loss= -64.01700 time= 0.68300\n",
      "Epoch: 0323 train_loss= -65.41000 time= 0.68400\n",
      "Epoch: 0324 train_loss= -66.31000 time= 0.67100\n",
      "Epoch: 0325 train_loss= -68.43400 time= 0.71700\n",
      "Epoch: 0326 train_loss= -70.44500 time= 0.70100\n",
      "Epoch: 0327 train_loss= -71.49300 time= 0.67100\n",
      "Epoch: 0328 train_loss= -73.01200 time= 0.68100\n",
      "Epoch: 0329 train_loss= -74.53700 time= 0.68800\n",
      "Epoch: 0330 train_loss= -75.40600 time= 0.67700\n",
      "Epoch: 0331 train_loss= -77.77200 time= 0.69400\n",
      "Epoch: 0332 train_loss= -78.50200 time= 0.68800\n",
      "Epoch: 0333 train_loss= -80.16300 time= 0.69500\n",
      "Epoch: 0334 train_loss= -82.57300 time= 0.68000\n",
      "Epoch: 0335 train_loss= -83.58000 time= 0.67200\n",
      "Epoch: 0336 train_loss= -85.10900 time= 0.67100\n",
      "Epoch: 0337 train_loss= -87.00100 time= 0.68600\n",
      "Epoch: 0338 train_loss= -88.76800 time= 0.71900\n",
      "Epoch: 0339 train_loss= -90.81600 time= 0.72700\n",
      "Epoch: 0340 train_loss= -91.71100 time= 0.73200\n",
      "Epoch: 0341 train_loss= -93.64200 time= 0.68900\n",
      "Epoch: 0342 train_loss= -94.68500 time= 0.70000\n",
      "Epoch: 0343 train_loss= -97.17200 time= 0.70200\n",
      "Epoch: 0344 train_loss= -97.94800 time= 0.67600\n",
      "Epoch: 0345 train_loss= -101.34800 time= 0.74900\n",
      "Epoch: 0346 train_loss= -102.29000 time= 0.81500\n",
      "Epoch: 0347 train_loss= -104.20100 time= 0.88000\n",
      "Epoch: 0348 train_loss= -106.92800 time= 0.77000\n",
      "Epoch: 0349 train_loss= -106.91500 time= 0.75000\n",
      "Epoch: 0350 train_loss= -108.97500 time= 0.79600\n",
      "Epoch: 0351 train_loss= -110.89700 time= 0.84800\n",
      "Epoch: 0352 train_loss= -113.34300 time= 0.84000\n",
      "Epoch: 0353 train_loss= -115.15400 time= 0.84000\n",
      "Epoch: 0354 train_loss= -116.12100 time= 0.80900\n",
      "Epoch: 0355 train_loss= -119.59500 time= 0.77000\n",
      "Epoch: 0356 train_loss= -120.43900 time= 0.73200\n",
      "Epoch: 0357 train_loss= -122.94700 time= 0.71800\n",
      "Epoch: 0358 train_loss= -124.76200 time= 0.73800\n",
      "Epoch: 0359 train_loss= -127.23400 time= 0.73800\n",
      "Epoch: 0360 train_loss= -129.07000 time= 0.73600\n",
      "Epoch: 0361 train_loss= -130.64900 time= 0.72700\n",
      "Epoch: 0362 train_loss= -132.13300 time= 0.76500\n",
      "Epoch: 0363 train_loss= -135.07300 time= 0.76000\n",
      "Epoch: 0364 train_loss= -137.08100 time= 0.73500\n",
      "Epoch: 0365 train_loss= -139.59300 time= 0.74200\n",
      "Epoch: 0366 train_loss= -140.28300 time= 0.76300\n",
      "Epoch: 0367 train_loss= -145.35500 time= 0.72200\n",
      "Epoch: 0368 train_loss= -145.47500 time= 0.72000\n",
      "Epoch: 0369 train_loss= -147.22100 time= 0.73000\n",
      "Epoch: 0370 train_loss= -149.13900 time= 0.72900\n",
      "Epoch: 0371 train_loss= -151.67400 time= 0.72900\n",
      "Epoch: 0372 train_loss= -154.01200 time= 0.74500\n",
      "Epoch: 0373 train_loss= -155.90500 time= 0.75600\n",
      "Epoch: 0374 train_loss= -158.50500 time= 0.75400\n",
      "Epoch: 0375 train_loss= -161.06200 time= 0.73600\n",
      "Epoch: 0376 train_loss= -163.07300 time= 0.72300\n",
      "Epoch: 0377 train_loss= -165.52400 time= 0.73400\n",
      "Epoch: 0378 train_loss= -168.97900 time= 0.74100\n",
      "Epoch: 0379 train_loss= -170.10000 time= 0.74800\n",
      "Epoch: 0380 train_loss= -173.55500 time= 0.77900\n",
      "Epoch: 0381 train_loss= -174.56900 time= 0.74700\n",
      "Epoch: 0382 train_loss= -177.45800 time= 0.72300\n",
      "Epoch: 0383 train_loss= -179.99900 time= 0.73700\n",
      "Epoch: 0384 train_loss= -182.30700 time= 0.73600\n",
      "Epoch: 0385 train_loss= -182.90500 time= 0.73400\n",
      "Epoch: 0386 train_loss= -188.27100 time= 0.69400\n",
      "Epoch: 0387 train_loss= -189.86300 time= 0.69000\n",
      "Epoch: 0388 train_loss= -190.38900 time= 0.68400\n",
      "Epoch: 0389 train_loss= -194.49100 time= 0.68100\n",
      "Epoch: 0390 train_loss= -197.68600 time= 0.67000\n",
      "Epoch: 0391 train_loss= -200.60200 time= 0.69400\n",
      "Epoch: 0392 train_loss= -202.14900 time= 0.69400\n",
      "Epoch: 0393 train_loss= -203.95400 time= 0.67500\n",
      "Epoch: 0394 train_loss= -206.96300 time= 0.71100\n",
      "Epoch: 0395 train_loss= -209.83000 time= 0.68700\n",
      "Epoch: 0396 train_loss= -212.95800 time= 0.67700\n",
      "Epoch: 0397 train_loss= -217.16800 time= 0.68200\n",
      "Epoch: 0398 train_loss= -219.02700 time= 0.68400\n",
      "Epoch: 0399 train_loss= -219.75800 time= 0.67700\n",
      "Epoch: 0400 train_loss= -222.84300 time= 0.68800\n",
      "Epoch: 0401 train_loss= -225.15400 time= 0.70800\n",
      "Epoch: 0402 train_loss= -228.60300 time= 0.67800\n",
      "Epoch: 0403 train_loss= -232.74200 time= 0.67300\n",
      "Epoch: 0404 train_loss= -233.01200 time= 0.67000\n",
      "Epoch: 0405 train_loss= -236.55300 time= 0.68200\n",
      "Epoch: 0406 train_loss= -239.66100 time= 0.69100\n",
      "Epoch: 0407 train_loss= -242.41600 time= 0.67700\n",
      "Epoch: 0408 train_loss= -245.28100 time= 0.69900\n",
      "Epoch: 0409 train_loss= -248.37800 time= 0.70100\n",
      "Epoch: 0410 train_loss= -253.17200 time= 0.69800\n",
      "Epoch: 0411 train_loss= -254.02700 time= 0.68700\n",
      "Epoch: 0412 train_loss= -257.10600 time= 0.69100\n",
      "Epoch: 0413 train_loss= -259.88900 time= 0.67600\n",
      "Epoch: 0414 train_loss= -262.09800 time= 0.68400\n",
      "Epoch: 0415 train_loss= -267.08800 time= 0.69800\n",
      "Epoch: 0416 train_loss= -271.12900 time= 0.69700\n",
      "Epoch: 0417 train_loss= -273.24800 time= 0.67800\n",
      "Epoch: 0418 train_loss= -275.35700 time= 0.68700\n",
      "Epoch: 0419 train_loss= -278.38200 time= 0.67500\n",
      "Epoch: 0420 train_loss= -281.77700 time= 0.67700\n",
      "Epoch: 0421 train_loss= -284.44000 time= 0.68000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adj, label, x \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# print(label, tf.cast(x, tf.float32),  tf.cast(adj, tf.float32))\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGVAE_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m avg_cost \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(adj_train))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(avg_cost, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t, \u001b[38;5;241m3\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[36], line 138\u001b[0m, in \u001b[0;36mOptimizerVAE.train_step\u001b[1;34m(self, y, x, adj, rate, model)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m    137\u001b[0m     cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y, x, adj\u001b[38;5;241m=\u001b[39madj, rate\u001b[38;5;241m=\u001b[39mrate, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(cost\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m    139\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(cost, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m    140\u001b[0m opt_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Initializing...\")\n",
    "# VAE model\n",
    "GVAE_model = GCNModelVAE(num_features, num_nodes, features_nonzero)\n",
    "# Optimizer\n",
    "optimizer = OptimizerVAE(model=GVAE_model, num_nodes=num_nodes,\n",
    "                            num_features=num_features, norm=norm)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "prev_cost = float(\"inf\")\n",
    "stop_val = 0\n",
    "stop_num = 10\n",
    "FLAGS_shuffle = False\n",
    "for epoch in range(1000):\n",
    "    t = time.time()\n",
    "    # Compute average loss\n",
    "    loss = 0\n",
    "    for adj, label, x in train_dataset:\n",
    "        # print(label, tf.cast(x, tf.float32),  tf.cast(adj, tf.float32))\n",
    "        loss += optimizer.train_step(label, \n",
    "                                     tf.cast(x, tf.float32), \n",
    "                                     tf.cast(adj, tf.float32), \n",
    "                                     dropout_rate, GVAE_model)\n",
    "\n",
    "    avg_cost = loss.numpy() / (len(adj_train))\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(round(avg_cost, 3)),\n",
    "            \"time=\", \"{:.5f}\".format(round(time.time() - t, 3)))\n",
    "    Computational_time[run] += (time.time() - t)\n",
    "    num_epoch[run] += 1\n",
    "\n",
    "    # When to stop the iteration\n",
    "    if (prev_cost < avg_cost):\n",
    "        stop_val += 1\n",
    "        if (stop_val == stop_num):\n",
    "            break\n",
    "    else:\n",
    "        stop_val = 0\n",
    "        prev_cost = avg_cost\n",
    "\n",
    "Computational_time[run] = Computational_time[run] / num_epoch[run]\n",
    "\n",
    "print(\"Time for each epoch:\", np.round(Computational_time[run], 3))\n",
    "\n",
    "meanr, logvarr = GVAE_model.encoder(tf.cast(features_train, tf.float32), tf.cast(adj_norm_train, tf.float32), 0.)\n",
    "ztr = GVAE_model.reparameterize(meanr, logvarr)\n",
    "meane, logvare = GVAE_model.encoder(tf.cast(features_test, tf.float32), tf.cast(adj_norm_test, tf.float32), 0.)\n",
    "zte = GVAE_model.reparameterize(meane, logvare)\n",
    "\n",
    "train_feature = deepcopy(ztr).numpy().reshape(len(ztr), -1)\n",
    "test_feature = deepcopy(zte).numpy().reshape(len(zte), -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RegeneronSTS-MbjA0bbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
